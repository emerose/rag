"""Embedding provider module for the RAG system.

This module provides functionality for generating embeddings from text,
with error handling and retry logic.
"""

import logging
from collections.abc import Callable
from typing import TypeAlias

from aiolimiter import AsyncLimiter
from langchain_core.embeddings import Embeddings
from langchain_openai import OpenAIEmbeddings
from openai import AsyncOpenAI

# Update OpenAI error imports for newer OpenAI library versions
try:
    # Try importing from older OpenAI versions (< 1.0.0)
    from openai.error import APIConnectionError, APIError, RateLimitError
except ImportError:
    # Use newer OpenAI imports (>= 1.0.0)
    from openai import APIConnectionError, APIError, RateLimitError

from tenacity import (
    AsyncRetrying,
    before_sleep_log,
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from rag.utils.logging_utils import log_message

logger = logging.getLogger(__name__)

# TypeAlias for log callback function
LogCallback: TypeAlias = Callable[[str, str, str], None]


class EmbeddingProvider:
    """Provides embedding generation functionality.

    This class encapsulates embedding generation with error handling and retry logic.
    """

    def __init__(
        self,
        model_name: str = "text-embedding-3-small",
        openai_api_key: str | None = None,
        *,  # Force keyword arguments for bool parameters
        show_progress_bar: bool = False,
        log_callback: LogCallback | None = None,
        requests_per_minute: int = 3000,
    ) -> None:
        """Initialize the embedding provider.

        Args:
            model_name: Name of the embedding model to use
            openai_api_key: OpenAI API key (optional if set in environment)
            show_progress_bar: Whether to show a progress bar for batch operations
            log_callback: Optional callback for logging

        """
        self.model_name = model_name
        self.openai_api_key = openai_api_key
        self.show_progress_bar = show_progress_bar
        self.log_callback = log_callback

        # Initialize the embeddings model
        self.embeddings = OpenAIEmbeddings(
            model=model_name,
            openai_api_key=openai_api_key,
            show_progress_bar=show_progress_bar,
        )
        self.async_client = AsyncOpenAI(api_key=openai_api_key)
        self.limiter = AsyncLimiter(requests_per_minute, time_period=60)

        # Store the model's embedding dimension
        self._embedding_dimension = self._get_embedding_dimension()

    def _log(self, level: str, message: str, task_id: str | None = None) -> None:
        """Log a message.

        Args:
            level: Log level (INFO, WARNING, ERROR, etc.)
            message: The log message

        """
        log_message(level, message, "Embeddings", self.log_callback, task_id)

    def _get_embedding_dimension(self) -> int:
        """Get the dimension of embeddings.

        Returns:
            Dimension of embeddings

        """
        try:
            # Generate a sample embedding to get the dimension
            self._log("DEBUG", "Getting embedding dimension from provider")
            embedding = self.embeddings.embed_query("sample text")
            self._log("DEBUG", f"Embedding dimension: {len(embedding)}")
        except (APIError, APIConnectionError, ValueError) as e:
            self._log("ERROR", f"Failed to determine embedding dimension: {e}")
            # Default dimensions for known models
            if "text-embedding-3" in self.model_name:
                return 1536  # Default for text-embedding-3-small/large
            if "text-embedding-ada-002" in self.model_name:
                return 1536  # Default for ada-002
            return 1024  # Fallback default
        else:
            return len(embedding)

    @property
    def embedding_dimension(self) -> int:
        """Get the dimension of the embeddings generated by the model.

        Returns:
            Dimension of the embeddings

        """
        return self._embedding_dimension

    @property
    def get_embeddings_model(self) -> Embeddings:
        """Get the underlying embeddings model.

        Returns:
            The langchain embeddings model

        """
        return self.embeddings

    @retry(
        retry=retry_if_exception_type((RateLimitError, APIError, APIConnectionError)),
        wait=wait_exponential(multiplier=1, min=1, max=60),
        stop=stop_after_attempt(8),
        before_sleep=before_sleep_log(logger, logging.WARNING),
    )
    def embed_texts(self, texts: list[str]) -> list[list[float]]:
        """Generate embeddings for a list of texts.

        Args:
            texts: List of texts to embed

        Returns:
            List of embeddings (lists of floats)

        Raises:
            ValueError, TypeError: If embedding generation fails
            RateLimitError, APIError, APIConnectionError: API errors that will be retried

        """
        if not texts:
            return []

        self._log("DEBUG", f"Generating embeddings for {len(texts)} texts")

        try:
            embeddings = self.embeddings.embed_documents(texts)
            self._log("DEBUG", f"Successfully embedded {len(texts)} texts")
        except (RateLimitError, APIError, APIConnectionError) as e:
            self._log(
                "WARNING",
                f"API error during embedding generation: {e}. Retrying...",
            )
            raise  # Let tenacity retry
        except (ValueError, TypeError) as e:
            self._log("ERROR", f"Failed to generate embeddings: {e}")
            raise
        else:
            return embeddings

    async def embed_texts_async(self, texts: list[str]) -> list[list[float]]:
        """Asynchronously generate embeddings for a list of texts."""

        if not texts:
            return []

        self._log("DEBUG", f"Generating embeddings for {len(texts)} texts")

        async for attempt in AsyncRetrying(
            retry=retry_if_exception_type(
                (RateLimitError, APIError, APIConnectionError)
            ),
            wait=wait_exponential(multiplier=1, min=1, max=60),
            stop=stop_after_attempt(8),
            before_sleep=before_sleep_log(logger, logging.WARNING),
        ):
            with attempt:
                async with self.limiter:
                    response = await self.async_client.embeddings.create(
                        model=self.model_name,
                        input=texts,
                    )
                self._log("DEBUG", f"Successfully embedded {len(texts)} texts")
                return [d.embedding for d in response.data]

    @retry(
        retry=retry_if_exception_type((RateLimitError, APIError, APIConnectionError)),
        wait=wait_exponential(multiplier=1, min=1, max=60),
        stop=stop_after_attempt(8),
        before_sleep=before_sleep_log(logger, logging.WARNING),
    )
    def embed_query(self, query: str) -> list[float]:
        """Generate embedding for a query.

        Args:
            query: Query text to embed

        Returns:
            Embedding for the query

        Raises:
            ValueError, TypeError: If embedding generation fails
            RateLimitError, APIError, APIConnectionError: API errors that will be retried

        """
        self._log("DEBUG", "Embedding query")

        try:
            embedding = self.embeddings.embed_query(query)
            self._log("DEBUG", "Successfully embedded query")
        except (RateLimitError, APIError, APIConnectionError) as e:
            self._log("WARNING", f"API error during query embedding: {e}. Retrying...")
            raise  # Let tenacity retry
        except (ValueError, TypeError) as e:
            self._log("ERROR", f"Failed to embed query: {e}")
            raise
        else:
            return embedding

    async def embed_query_async(self, query: str) -> list[float]:
        """Asynchronously generate embedding for a query."""

        self._log("DEBUG", "Embedding query")

        async for attempt in AsyncRetrying(
            retry=retry_if_exception_type(
                (RateLimitError, APIError, APIConnectionError)
            ),
            wait=wait_exponential(multiplier=1, min=1, max=60),
            stop=stop_after_attempt(8),
            before_sleep=before_sleep_log(logger, logging.WARNING),
        ):
            with attempt:
                async with self.limiter:
                    response = await self.async_client.embeddings.create(
                        model=self.model_name,
                        input=[query],
                    )
                self._log("DEBUG", "Successfully embedded query")
                return response.data[0].embedding

    def get_model_info(self) -> dict[str, str]:
        """Get information about the embeddings model.

        Returns:
            Dictionary with embedding model information

        """
        return {
            "embedding_model": self.model_name,
            "model_version": self._get_model_version(),
            "embedding_dimension": str(self.embedding_dimension),
        }

    def _get_model_version(self) -> str:
        """Get the version of the embedding model.

        Returns:
            Model version string

        """
        # For OpenAI models, derive version from the model name
        if self.model_name == "text-embedding-3-small":
            return "3-small"
        if self.model_name == "text-embedding-3-large":
            return "3-large"
        if self.model_name == "text-embedding-ada-002":
            return "ada-002"
        return "unknown"
