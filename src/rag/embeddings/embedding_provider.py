"""Embedding provider module for the RAG system.

This module provides functionality for generating embeddings from text,
with error handling and retry logic.
"""

import logging
from typing import Any, Dict, List, Optional, Union

from langchain_core.embeddings import Embeddings
from langchain_openai import OpenAIEmbeddings

# Update OpenAI error imports for newer OpenAI library versions
try:
    # Try importing from older OpenAI versions (< 1.0.0)
    from openai.error import APIError, APIConnectionError, RateLimitError
except ImportError:
    # Use newer OpenAI imports (>= 1.0.0)
    from openai import APIError, APIConnectionError, RateLimitError

from tenacity import (
    before_sleep_log,
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from ..utils.logging_utils import log_message

logger = logging.getLogger(__name__)


class EmbeddingProvider:
    """Provides embedding generation functionality.
    
    This class encapsulates embedding generation with error handling and retry logic.
    """
    
    def __init__(self,
                 model_name: str = "text-embedding-3-small",
                 openai_api_key: Optional[str] = None,
                 show_progress_bar: bool = False,
                 log_callback: Optional[Any] = None) -> None:
        """Initialize the embedding provider.
        
        Args:
            model_name: Name of the embedding model to use
            openai_api_key: OpenAI API key (optional if set in environment)
            show_progress_bar: Whether to show a progress bar for batch operations
            log_callback: Optional callback for logging
        """
        self.model_name = model_name
        self.openai_api_key = openai_api_key
        self.show_progress_bar = show_progress_bar
        self.log_callback = log_callback
        
        # Initialize the embeddings model
        self.embeddings = OpenAIEmbeddings(
            model=model_name,
            openai_api_key=openai_api_key,
            show_progress_bar=show_progress_bar,
        )
        
        # Store the model's embedding dimension
        self._embedding_dimension = self._get_embedding_dimension()
        
    def _log(self, level: str, message: str) -> None:
        """Log a message.
        
        Args:
            level: Log level (INFO, WARNING, ERROR, etc.)
            message: The log message
        """
        log_message(level, message, "Embeddings", self.log_callback)
        
    def _get_embedding_dimension(self) -> int:
        """Get the dimension of the embeddings generated by the model.
        
        Returns:
            Dimension of the embeddings
        """
        # Embed a simple string to determine the embedding dimension
        try:
            embedding = self.embed_texts(["Test to determine embedding dimension"])[0]
            self._log("DEBUG", f"Embedding dimension: {len(embedding)}")
            return len(embedding)
        except Exception as e:
            self._log("ERROR", f"Failed to determine embedding dimension: {e}")
            # Default dimensions for known models
            if self.model_name == "text-embedding-3-small":
                return 1536
            elif self.model_name == "text-embedding-3-large":
                return 3072
            elif self.model_name == "text-embedding-ada-002":
                return 1536
            else:
                return 1536  # Default to 1536-dimensional embeddings
                
    @property
    def embedding_dimension(self) -> int:
        """Get the dimension of the embeddings generated by the model.
        
        Returns:
            Dimension of the embeddings
        """
        return self._embedding_dimension
        
    @property
    def get_embeddings_model(self) -> Embeddings:
        """Get the underlying embeddings model.
        
        Returns:
            The langchain embeddings model
        """
        return self.embeddings
        
    @retry(
        retry=retry_if_exception_type((RateLimitError, APIError, APIConnectionError)),
        wait=wait_exponential(multiplier=1, min=1, max=60),
        stop=stop_after_attempt(8),
        before_sleep=before_sleep_log(logger, logging.WARNING)
    )
    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for a list of texts.
        
        Args:
            texts: List of texts to embed
            
        Returns:
            List of embeddings (lists of floats)
            
        Raises:
            Exception: If embedding generation fails after retries
        """
        if not texts:
            return []
            
        self._log("DEBUG", f"Generating embeddings for {len(texts)} texts")
        
        try:
            embeddings = self.embeddings.embed_documents(texts)
            self._log("DEBUG", f"Successfully embedded {len(texts)} texts")
            return embeddings
            
        except (RateLimitError, APIError, APIConnectionError) as e:
            self._log("WARNING", f"API error during embedding generation: {e}. Retrying...")
            raise  # Let tenacity retry
            
        except Exception as e:
            self._log("ERROR", f"Failed to generate embeddings: {e}")
            raise
            
    @retry(
        retry=retry_if_exception_type((RateLimitError, APIError, APIConnectionError)),
        wait=wait_exponential(multiplier=1, min=1, max=60),
        stop=stop_after_attempt(8),
        before_sleep=before_sleep_log(logger, logging.WARNING)
    )
    def embed_query(self, query: str) -> List[float]:
        """Generate embedding for a query string.
        
        Args:
            query: Query string to embed
            
        Returns:
            Embedding vector as list of floats
            
        Raises:
            Exception: If embedding generation fails after retries
        """
        self._log("DEBUG", f"Generating embedding for query")
        
        try:
            embedding = self.embeddings.embed_query(query)
            self._log("DEBUG", "Successfully embedded query")
            return embedding
            
        except (RateLimitError, APIError, APIConnectionError) as e:
            self._log("WARNING", f"API error during query embedding: {e}. Retrying...")
            raise  # Let tenacity retry
            
        except Exception as e:
            self._log("ERROR", f"Failed to generate query embedding: {e}")
            raise
            
    def get_model_info(self) -> Dict[str, str]:
        """Get information about the embeddings model.
        
        Returns:
            Dictionary with embedding model information
        """
        return {
            "embedding_model": self.model_name,
            "model_version": self._get_model_version(),
            "embedding_dimension": str(self.embedding_dimension)
        }
        
    def _get_model_version(self) -> str:
        """Get the version of the embedding model.
        
        Returns:
            Model version string
        """
        # For OpenAI models, derive version from the model name
        if self.model_name == "text-embedding-3-small":
            return "3-small"
        elif self.model_name == "text-embedding-3-large":
            return "3-large"
        elif self.model_name == "text-embedding-ada-002":
            return "ada-002"
        else:
            return "unknown" 
