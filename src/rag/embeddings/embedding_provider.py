"""Embedding provider module for the RAG system.

This module provides a high-level embedding provider that wraps the core
EmbeddingService with additional functionality and maintains backward compatibility.
"""

import logging
from collections.abc import Callable
from typing import TYPE_CHECKING, TypeAlias

from langchain_core.embeddings import Embeddings

from rag.utils.logging_utils import log_message

from .protocols import EmbeddingServiceProtocol

if TYPE_CHECKING:
    from .embedding_service import EmbeddingService

logger = logging.getLogger(__name__)

# TypeAlias for log callback function
LogCallback: TypeAlias = Callable[[str, str, str], None]


class EmbeddingProvider(EmbeddingServiceProtocol):
    """High-level embedding provider that wraps EmbeddingService.

    This class provides backward compatibility and additional functionality
    while delegating core embedding operations to the focused EmbeddingService.
    Implements the EmbeddingServiceProtocol for dependency injection compatibility.
    """

    def __init__(
        self,
        model_name: str = "text-embedding-3-small",
        openai_api_key: str | None = None,
        *,  # Force keyword arguments for bool parameters
        show_progress_bar: bool = False,
        log_callback: LogCallback | None = None,
        embedding_service: "EmbeddingService | None" = None,
    ) -> None:
        """Initialize the embedding provider.

        Args:
            model_name: Name of the embedding model to use
            openai_api_key: OpenAI API key (optional if set in environment)
            show_progress_bar: Whether to show a progress bar for batch operations
            log_callback: Optional callback for logging
            embedding_service: Optional pre-configured EmbeddingService instance

        """
        self.model_name = model_name
        self.openai_api_key = openai_api_key
        self.show_progress_bar = show_progress_bar
        self.log_callback = log_callback

        # Initialize or use provided embedding service
        if embedding_service is not None:
            self._embedding_service = embedding_service
        else:
            from .embedding_service import EmbeddingService

            self._embedding_service = EmbeddingService(
                model_name=model_name,
                openai_api_key=openai_api_key,
                show_progress_bar=show_progress_bar,
                log_callback=log_callback,
            )

    def _log(self, level: str, message: str) -> None:
        """Log a message.

        Args:
            level: Log level (INFO, WARNING, ERROR, etc.)
            message: The log message

        """
        log_message(level, message, "EmbeddingProvider", self.log_callback)

    @property
    def embedding_dimension(self) -> int:
        """Get the dimension of the embeddings generated by the model.

        Returns:
            Dimension of the embeddings

        """
        return self._embedding_service.embedding_dimension

    @property
    def get_embeddings_model(self) -> Embeddings:
        """Get the underlying embeddings model.

        Returns:
            The langchain embeddings model

        """
        return self._embedding_service.underlying_model

    def embed_texts(self, texts: list[str]) -> list[list[float]]:
        """Generate embeddings for a list of texts.

        Args:
            texts: List of texts to embed

        Returns:
            List of embeddings (lists of floats)

        Raises:
            ValueError, TypeError: If embedding generation fails
            RateLimitError, APIError, APIConnectionError: API errors that will be retried

        """
        self._log("DEBUG", f"Delegating embedding of {len(texts)} texts to service")
        return self._embedding_service.embed_texts(texts)

    def embed_query(self, query: str) -> list[float]:
        """Generate embedding for a query.

        Args:
            query: Query text to embed

        Returns:
            Embedding for the query

        Raises:
            ValueError, TypeError: If embedding generation fails
            RateLimitError, APIError, APIConnectionError: API errors that will be retried

        """
        self._log("DEBUG", "Delegating query embedding to service")
        return self._embedding_service.embed_query(query)

    def get_model_info(self) -> dict[str, str]:
        """Get information about the embeddings model.

        Returns:
            Dictionary with embedding model information

        """
        return {
            "embedding_model": self.model_name,
            "model_version": self._get_model_version(),
            "embedding_dimension": str(self.embedding_dimension),
        }

    def _get_model_version(self) -> str:
        """Get the version of the embedding model.

        Returns:
            Model version string

        """
        # For OpenAI models, derive version from the model name
        if self.model_name == "text-embedding-3-small":
            return "3-small"
        if self.model_name == "text-embedding-3-large":
            return "3-large"
        if self.model_name == "text-embedding-ada-002":
            return "ada-002"
        return "unknown"
