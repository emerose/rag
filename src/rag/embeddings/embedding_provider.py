"""Embedding provider module for the RAG system.

This module provides a high-level embedding provider that wraps the core
EmbeddingService with additional functionality and maintains backward compatibility.
"""

import logging
from collections.abc import Callable
from typing import TYPE_CHECKING, TypeAlias

from langchain_core.embeddings import Embeddings

from rag.config.components import EmbeddingConfig
from rag.utils.logging_utils import log_message

from .protocols import EmbeddingServiceProtocol

if TYPE_CHECKING:
    from .embedding_service import EmbeddingService

logger = logging.getLogger(__name__)

# TypeAlias for log callback function
LogCallback: TypeAlias = Callable[[str, str, str], None]


class EmbeddingProvider(EmbeddingServiceProtocol, Embeddings):
    """High-level embedding provider that wraps EmbeddingService.

    This class provides backward compatibility and additional functionality
    while delegating core embedding operations to the focused EmbeddingService.
    Implements the EmbeddingServiceProtocol for dependency injection compatibility.
    """

    def __init__(
        self,
        config: EmbeddingConfig | None = None,
        *,
        model_name: str | None = None,
        openai_api_key: str | None = None,
        show_progress_bar: bool = False,
        log_callback: LogCallback | None = None,
        embedding_service: "EmbeddingService | None" = None,
    ) -> None:
        """Initialize the embedding provider.

        Args:
            config: Embedding configuration (preferred way to configure)
            model_name: [DEPRECATED] Name of the embedding model to use
            openai_api_key: OpenAI API key (optional if set in environment)
            show_progress_bar: Whether to show a progress bar for batch operations
            log_callback: Optional callback for logging
            embedding_service: Optional pre-configured EmbeddingService instance

        Note:
            If config is provided, it takes precedence over individual parameters.
            Individual parameters are maintained for backward compatibility.
        """
        # Use config if provided, otherwise use individual parameters
        if config is not None:
            self.config = config
            self.model_name = config.model
            self.openai_api_key = openai_api_key  # Keep API key separate for security
        else:
            # Backward compatibility: create config from individual parameters
            self.config = EmbeddingConfig(
                model=model_name or "text-embedding-3-small",
                # Note: Other config parameters use defaults from EmbeddingConfig
            )
            self.model_name = self.config.model
            self.openai_api_key = openai_api_key

        self.show_progress_bar = show_progress_bar
        self.log_callback = log_callback

        # Initialize or use provided embedding service
        if embedding_service is not None:
            self._embedding_service = embedding_service
        else:
            from .embedding_service import EmbeddingService, RetryConfig

            # Create retry config from embedding config
            retry_config = RetryConfig(
                max_retries=self.config.max_retries,
                # Keep other retry settings as defaults for now
            )

            self._embedding_service = EmbeddingService(
                model_name=self.model_name,
                openai_api_key=openai_api_key,
                show_progress_bar=show_progress_bar,
                log_callback=log_callback,
                retry_config=retry_config,
            )

    def _log(self, level: str, message: str) -> None:
        """Log a message.

        Args:
            level: Log level (INFO, WARNING, ERROR, etc.)
            message: The log message

        """
        log_message(level, message, "EmbeddingProvider", self.log_callback)

    @property
    def embedding_dimension(self) -> int:
        """Get the dimension of the embeddings generated by the model.

        Returns:
            Dimension of the embeddings

        """
        return self._embedding_service.embedding_dimension

    @property
    def get_embeddings_model(self) -> Embeddings:
        """Get the underlying embeddings model.

        Returns:
            The langchain embeddings model

        """
        return self._embedding_service.underlying_model

    def embed_texts(self, texts: list[str]) -> list[list[float]]:
        """Generate embeddings for a list of texts.

        Args:
            texts: List of texts to embed

        Returns:
            List of embeddings (lists of floats)

        Raises:
            ValueError, TypeError: If embedding generation fails
            RateLimitError, APIError, APIConnectionError: API errors that will be retried

        """
        self._log("DEBUG", f"Delegating embedding of {len(texts)} texts to service")
        return self._embedding_service.embed_texts(texts)

    def embed_query(self, query: str) -> list[float]:
        """Generate embedding for a query.

        Args:
            query: Query text to embed

        Returns:
            Embedding for the query

        Raises:
            ValueError, TypeError: If embedding generation fails
            RateLimitError, APIError, APIConnectionError: API errors that will be retried

        """
        self._log("DEBUG", "Delegating query embedding to service")
        return self._embedding_service.embed_query(query)

    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        """Generate embeddings for a list of documents.

        Args:
            texts: List of texts to embed

        Returns:
            List of embeddings for the texts

        Raises:
            ValueError, TypeError: If embedding generation fails
            RateLimitError, APIError, APIConnectionError: API errors that will be retried

        """
        self._log("DEBUG", f"Delegating embedding of {len(texts)} documents to service")
        return self._embedding_service.embed_texts(texts)

    def get_model_info(self) -> dict[str, str]:
        """Get information about the embeddings model.

        Returns:
            Dictionary with embedding model information

        """
        return {
            "embedding_model": self.model_name,
            "model_version": self._get_model_version(),
            "embedding_dimension": str(self.embedding_dimension),
        }

    def _get_model_version(self) -> str:
        """Get the version of the embedding model.

        Returns:
            Model version string

        """
        # For OpenAI models, derive version from the model name
        if self.model_name == "text-embedding-3-small":
            return "3-small"
        if self.model_name == "text-embedding-3-large":
            return "3-large"
        if self.model_name == "text-embedding-ada-002":
            return "ada-002"
        return "unknown"
