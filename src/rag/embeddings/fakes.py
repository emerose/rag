"""Fake implementations for testing embedding components."""

from __future__ import annotations

import hashlib

from .protocols import EmbeddingServiceProtocol


class FakeEmbeddingService(EmbeddingServiceProtocol):
    """Fake embedding service implementation for testing.

    This fake implementation provides deterministic embedding outputs
    without making HTTP calls, enabling fast and reliable unit tests.
    The embeddings are generated using deterministic hashing of the input text.
    """

    def __init__(
        self,
        embedding_dimension: int = 384,
        model_name: str = "fake-embedding-model",
        model_version: str = "1.0.0",
    ) -> None:
        """Initialize the fake embedding service.

        Args:
            embedding_dimension: Dimension of the generated embeddings
            model_name: Name of the fake model
            model_version: Version of the fake model
        """
        self._embedding_dimension = embedding_dimension
        self._model_name = model_name
        self._model_version = model_version

    @property
    def embedding_dimension(self) -> int:
        """Get the dimension of the embeddings generated by the service.

        Returns:
            Dimension of the embeddings
        """
        return self._embedding_dimension

    def embed_texts(self, texts: list[str]) -> list[list[float]]:
        """Generate embeddings for a list of texts.

        Args:
            texts: List of texts to embed

        Returns:
            List of deterministic embeddings based on text content

        Raises:
            ValueError: If texts list is empty or contains invalid content
        """
        if not texts:
            raise ValueError("Cannot embed empty text list")

        embeddings = []
        for text in texts:
            if not isinstance(text, str):
                raise ValueError(f"Text must be a string, got {type(text)}")
            embedding = self._generate_deterministic_embedding(text)
            embeddings.append(embedding)

        return embeddings

    def embed_query(self, query: str) -> list[float]:
        """Generate embedding for a query.

        Args:
            query: Query text to embed

        Returns:
            Deterministic embedding based on query content

        Raises:
            ValueError: If query is invalid
        """
        if not isinstance(query, str):
            raise ValueError(f"Query must be a string, got {type(query)}")

        if not query.strip():
            raise ValueError("Query cannot be empty or whitespace-only")

        return self._generate_deterministic_embedding(query)

    def get_model_info(self) -> dict[str, str]:
        """Get information about the embeddings model.

        Returns:
            Dictionary with fake embedding model information
        """
        return {
            "model_name": self._model_name,
            "model_version": self._model_version,
            "dimension": str(self._embedding_dimension),
            "type": "fake",
            "provider": "test",
        }

    def _generate_deterministic_embedding(self, text: str) -> list[float]:
        """Generate a deterministic embedding for the given text.

        This method creates consistent embeddings by:
        1. Hashing the text to get a deterministic seed
        2. Using the seed to generate pseudo-random values
        3. Normalizing to create a unit vector

        Args:
            text: Input text to generate embedding for

        Returns:
            Deterministic embedding vector of specified dimension
        """
        # Create a deterministic seed from the text
        text_hash = hashlib.sha256(text.encode("utf-8")).digest()

        # Generate embedding values using the hash bytes
        embedding = []
        for i in range(self._embedding_dimension):
            # Use different byte positions cyclically to create variation
            byte_index = i % len(text_hash)
            # Convert byte to float in range [-1, 1]
            normalized_value = (text_hash[byte_index] - 127.5) / 127.5
            embedding.append(normalized_value)

        # Normalize the vector to unit length for more realistic embeddings
        magnitude = sum(x * x for x in embedding) ** 0.5
        if magnitude > 0:
            embedding = [x / magnitude for x in embedding]

        return embedding


class DeterministicEmbeddingService(EmbeddingServiceProtocol):
    """Deterministic embedding service with configurable outputs.

    This implementation allows you to pre-configure specific embeddings
    for specific texts, useful for very controlled testing scenarios.
    """

    def __init__(
        self,
        embedding_dimension: int = 384,
        model_name: str = "deterministic-model",
        predefined_embeddings: dict[str, list[float]] | None = None,
    ) -> None:
        """Initialize the deterministic embedding service.

        Args:
            embedding_dimension: Dimension of the generated embeddings
            model_name: Name of the model
            predefined_embeddings: Dictionary mapping texts to specific embeddings
        """
        self._embedding_dimension = embedding_dimension
        self._model_name = model_name
        self._predefined_embeddings = predefined_embeddings or {}

        # Validate predefined embeddings have correct dimensions
        for text, embedding in self._predefined_embeddings.items():
            if len(embedding) != embedding_dimension:
                raise ValueError(
                    f"Predefined embedding for '{text}' has dimension {len(embedding)}, "
                    f"expected {embedding_dimension}"
                )

    @property
    def embedding_dimension(self) -> int:
        """Get the dimension of the embeddings generated by the service."""
        return self._embedding_dimension

    def embed_texts(self, texts: list[str]) -> list[list[float]]:
        """Generate embeddings for a list of texts."""
        if not texts:
            raise ValueError("Cannot embed empty text list")

        embeddings = []
        for text in texts:
            embedding = self.embed_query(text)
            embeddings.append(embedding)

        return embeddings

    def embed_query(self, query: str) -> list[float]:
        """Generate embedding for a query."""
        if not isinstance(query, str):
            raise ValueError(f"Query must be a string, got {type(query)}")

        if not query.strip():
            raise ValueError("Query cannot be empty or whitespace-only")

        # Use predefined embedding if available
        if query in self._predefined_embeddings:
            return self._predefined_embeddings[query].copy()

        # Fall back to deterministic generation
        return self._generate_simple_embedding(query)

    def get_model_info(self) -> dict[str, str]:
        """Get information about the embeddings model."""
        return {
            "model_name": self._model_name,
            "model_version": "1.0.0",
            "dimension": str(self._embedding_dimension),
            "type": "deterministic",
            "provider": "test",
            "predefined_count": str(len(self._predefined_embeddings)),
        }

    def add_predefined_embedding(self, text: str, embedding: list[float]) -> None:
        """Add a predefined embedding for specific text.

        Args:
            text: Text to map to the embedding
            embedding: Embedding vector to return for this text

        Raises:
            ValueError: If embedding dimension doesn't match
        """
        if len(embedding) != self._embedding_dimension:
            raise ValueError(
                f"Embedding dimension {len(embedding)} doesn't match "
                f"expected {self._embedding_dimension}"
            )

        self._predefined_embeddings[text] = embedding.copy()

    def _generate_simple_embedding(self, text: str) -> list[float]:
        """Generate a simple deterministic embedding."""
        # Create a simple pattern based on text length and content
        text_sum = sum(ord(c) for c in text)
        pattern = text_sum % 100  # Simple pattern generator

        embedding = []
        for i in range(self._embedding_dimension):
            # Create a simple sine-wave pattern
            value = (pattern + i) / 100.0
            embedding.append(value)

        # Normalize to unit vector
        magnitude = sum(x * x for x in embedding) ** 0.5
        if magnitude > 0:
            embedding = [x / magnitude for x in embedding]

        return embedding
