"""Embedding service module for the RAG system.

This module provides a focused embedding service that handles embedding generation
with error handling and retry logic. It's designed to be used as a dependency
by higher-level components like EmbeddingProvider.
"""

import logging
from collections.abc import Callable
from dataclasses import dataclass
from typing import Any

from langchain_openai import OpenAIEmbeddings

# Import OpenAI errors (requires OpenAI >= 1.0.0)
from openai import APIConnectionError, APIError, RateLimitError
from tenacity import (
    before_sleep_log,
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from rag.utils.exceptions import EmbeddingGenerationError
from rag.utils.logging_utils import log_message

logger = logging.getLogger(__name__)

# Type alias for log callback function
type LogCallback = Callable[[str, str, str], None]


@dataclass
class RetryConfig:
    """Configuration for retry behavior in embedding operations."""

    max_retries: int = 8
    base_delay: float = 1.0  # Base delay in seconds
    max_delay: float = 60.0  # Maximum delay in seconds


class OpenAIEmbeddingService:
    """Core embedding service focused on embedding generation with retries.

    This service handles the low-level embedding generation with comprehensive
    error handling and retry logic. It's designed to be lightweight and focused
    on a single responsibility: generating embeddings reliably.
    """

    def __init__(
        self,
        model_name: str = "text-embedding-3-small",
        openai_api_key: str | None = None,
        *,  # Force keyword arguments for bool parameters
        show_progress_bar: bool = False,
        log_callback: LogCallback | None = None,
        retry_config: RetryConfig | None = None,
    ) -> None:
        """Initialize the embedding service.

        Args:
            model_name: Name of the embedding model to use
            openai_api_key: OpenAI API key (optional if set in environment)
            show_progress_bar: Whether to show a progress bar for batch operations
            log_callback: Optional callback for logging
            retry_config: Configuration for retry behavior (uses default if None)
        """
        self.model_name = model_name
        self.openai_api_key = openai_api_key
        self.show_progress_bar = show_progress_bar
        self.log_callback = log_callback
        self.retry_config = retry_config or RetryConfig()

        # Initialize the underlying embeddings model
        # Set API key via environment if provided
        if openai_api_key:
            import os

            os.environ["OPENAI_API_KEY"] = openai_api_key

        self._embeddings = OpenAIEmbeddings(
            model=model_name,
            show_progress_bar=show_progress_bar,
        )

        # Cache the embedding dimension
        self._embedding_dimension = self._determine_embedding_dimension()

    def _log(self, level: str, message: str) -> None:
        """Log a message using the configured callback or standard logging.

        Args:
            level: Log level (DEBUG, INFO, WARNING, ERROR, etc.)
            message: The log message
        """
        log_message(level, message, "OpenAIEmbeddingService", self.log_callback)

    def _determine_embedding_dimension(self) -> int:
        """Determine the embedding dimension for the configured model.

        Returns:
            Dimension of embeddings for this model

        Note:
            Uses a test embedding to determine dimension, with fallbacks
            for known models if the test fails.
        """
        try:
            self._log("DEBUG", "Determining embedding dimension from model")
            test_embedding = self._embeddings.embed_query("test")
            dimension = len(test_embedding)
            self._log("DEBUG", f"Embedding dimension determined: {dimension}")
            return dimension
        except (APIError, APIConnectionError, ValueError) as e:
            self._log("WARNING", f"Failed to determine embedding dimension: {e}")
            # Return known defaults for common models
            if "text-embedding-3" in self.model_name:
                return 1536  # Default for text-embedding-3-small/large
            if "text-embedding-ada-002" in self.model_name:
                return 1536  # Default for ada-002
            return 1024  # Fallback default

    @property
    def embedding_dimension(self) -> int:
        """Get the dimension of embeddings generated by this service.

        Returns:
            Dimension of the embeddings
        """
        return self._embedding_dimension

    @property
    def underlying_model(self) -> "OpenAIEmbeddings":
        """Get the underlying LangChain embeddings model.

        Returns:
            The LangChain OpenAIEmbeddings model instance
        """
        return self._embeddings

    @property
    def model_info(self) -> dict[str, str]:
        """Get information about the embedding model.

        Returns:
            Dictionary containing model information
        """
        return {
            "model_name": self.model_name,
            "embedding_dimension": str(self._embedding_dimension),
            "provider": "openai",
        }

    def _create_retry_decorator(
        self,
    ) -> Callable[[Callable[..., Any]], Callable[..., Any]]:
        """Create a retry decorator with the configured parameters."""
        return retry(
            retry=retry_if_exception_type(
                (RateLimitError, APIError, APIConnectionError)
            ),
            wait=wait_exponential(
                multiplier=1,
                min=self.retry_config.base_delay,
                max=self.retry_config.max_delay,
            ),
            stop=stop_after_attempt(self.retry_config.max_retries),
            before_sleep=before_sleep_log(logger, logging.WARNING),
        )

    def embed_texts(self, texts: list[str]) -> list[list[float]]:
        """Generate embeddings for multiple texts with retry logic.

        Args:
            texts: List of texts to embed

        Returns:
            List of embeddings (each embedding is a list of floats)

        Raises:
            EmbeddingGenerationError: If texts is empty or embedding generation fails
            RateLimitError, APIError, APIConnectionError: After max retries exceeded
        """
        if not texts:
            raise EmbeddingGenerationError(message="Cannot embed empty text list")

        # Validate all texts are non-empty
        for i, text in enumerate(texts):
            if not text.strip():
                raise EmbeddingGenerationError(
                    text=text,
                    message=f"Text at index {i} cannot be empty",
                )

        # Create and apply retry decorator
        retry_decorator = self._create_retry_decorator()
        embed_func = retry_decorator(self._embed_texts_impl)

        return embed_func(texts)

    def _embed_texts_impl(self, texts: list[str]) -> list[list[float]]:
        """Implementation of text embedding with error handling.

        Args:
            texts: List of texts to embed

        Returns:
            List of embeddings

        Raises:
            RateLimitError, APIError, APIConnectionError: API errors for retry
            ValueError, TypeError: Non-retryable errors
        """
        self._log("DEBUG", f"Generating embeddings for {len(texts)} texts")

        try:
            embeddings = self._embeddings.embed_documents(texts)
            self._log("DEBUG", f"Successfully embedded {len(texts)} texts")
            return embeddings
        except (RateLimitError, APIError, APIConnectionError) as e:
            self._log(
                "WARNING", f"API error during embedding generation: {e}. Retrying..."
            )
            raise  # Let tenacity handle the retry
        except (ValueError, TypeError) as e:
            self._log("ERROR", f"Non-retryable error in embedding generation: {e}")
            raise

    def embed_query(self, query: str) -> list[float]:
        """Generate embedding for a single query with retry logic.

        Args:
            query: Query text to embed

        Returns:
            Embedding for the query (list of floats)

        Raises:
            EmbeddingGenerationError: If query is empty or embedding generation fails
            RateLimitError, APIError, APIConnectionError: After max retries exceeded
        """
        if not query.strip():
            raise EmbeddingGenerationError(
                text=query, message="Cannot embed empty query"
            )

        # Create and apply retry decorator
        retry_decorator = self._create_retry_decorator()
        embed_func = retry_decorator(self._embed_query_impl)

        return embed_func(query)

    def _embed_query_impl(self, query: str) -> list[float]:
        """Implementation of query embedding with error handling.

        Args:
            query: Query text to embed

        Returns:
            Embedding for the query

        Raises:
            RateLimitError, APIError, APIConnectionError: API errors for retry
            ValueError, TypeError: Non-retryable errors
        """
        self._log("DEBUG", "Generating embedding for query")

        try:
            embedding = self._embeddings.embed_query(query)
            self._log("DEBUG", "Successfully embedded query")
            return embedding
        except (RateLimitError, APIError, APIConnectionError) as e:
            self._log("WARNING", f"API error during query embedding: {e}. Retrying...")
            raise  # Let tenacity handle the retry
        except (ValueError, TypeError) as e:
            self._log("ERROR", f"Non-retryable error in query embedding: {e}")
            raise
