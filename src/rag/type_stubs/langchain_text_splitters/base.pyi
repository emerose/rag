"""
This type stub file was generated by pyright.
"""

from abc import ABC, abstractmethod
from collections.abc import Callable, Collection, Iterable
from collections.abc import Set as AbstractSet
from dataclasses import dataclass
from enum import Enum
from typing import Any, Literal, TypeVar

from langchain_core.documents import Document
from typing import Protocol

logger = ...
TS = TypeVar("TS", bound="TextSplitter")

class BaseDocumentTransformer(Protocol):
    """Base protocol for document transformers."""
    def transform_documents(self, documents: list[Document]) -> list[Document]: ...

class TextSplitter(BaseDocumentTransformer, ABC):
    """Interface for splitting text into chunks."""
    def __init__(
        self,
        chunk_size: int = ...,
        chunk_overlap: int = ...,
        length_function: Callable[[str], int] = ...,
        keep_separator: bool | Literal["start", "end"] = ...,
        add_start_index: bool = ...,
        strip_whitespace: bool = ...,
    ) -> None:
        """Create a new TextSplitter.

        Args:
            chunk_size: Maximum size of chunks to return
            chunk_overlap: Overlap in characters between chunks
            length_function: Function that measures the length of given chunks
            keep_separator: Whether to keep the separator and where to place it
                            in each corresponding chunk (True='start')
            add_start_index: If `True`, includes chunk's start index in metadata
            strip_whitespace: If `True`, strips whitespace from the start and end of
                              every document
        """
        ...

    @abstractmethod
    def split_text(self, text: str) -> list[str]:
        """Split text into multiple components."""
        ...

    def create_documents(
        self, texts: list[str], metadatas: list[dict[str, Any]] | None = ...
    ) -> list[Document]:
        """Create documents from a list of texts."""
        ...

    def split_documents(self, documents: Iterable[Document]) -> list[Document]:
        """Split documents."""
        ...

    @classmethod
    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:
        """Text splitter that uses HuggingFace tokenizer to count length."""
        ...

    @classmethod
    def from_tiktoken_encoder(
        cls: type[TS],
        encoding_name: str = ...,
        model_name: str | None = ...,
        allowed_special: Literal["all"] | AbstractSet[str] = ...,
        disallowed_special: Literal["all"] | Collection[str] = ...,
        **kwargs: Any,
    ) -> TS:
        """Text splitter that uses tiktoken encoder to count length."""
        ...

    def transform_documents(
        self, documents: list[Document], **kwargs: Any
    ) -> list[Document]:
        """Transform sequence of documents by splitting them."""
        ...

class TokenTextSplitter(TextSplitter):
    """Splitting text to tokens using model tokenizer."""
    def __init__(
        self,
        encoding_name: str = ...,
        model_name: str | None = ...,
        allowed_special: Literal["all"] | AbstractSet[str] = ...,
        disallowed_special: Literal["all"] | Collection[str] = ...,
        **kwargs: Any,
    ) -> None:
        """Create a new TextSplitter."""
        ...

    def split_text(self, text: str) -> list[str]:
        """Splits the input text into smaller chunks based on tokenization.

        This method uses a custom tokenizer configuration to encode the input text
        into tokens, processes the tokens in chunks of a specified size with overlap,
        and decodes them back into text chunks. The splitting is performed using the
        `split_text_on_tokens` function.

        Args:
            text (str): The input text to be split into smaller chunks.

        Returns:
            List[str]: A list of text chunks, where each chunk is derived from a portion
            of the input text based on the tokenization and chunking rules.
        """
        ...

class Language(str, Enum):
    """Enum of the programming languages."""

    CPP = ...
    GO = ...
    JAVA = ...
    KOTLIN = ...
    JS = ...
    TS = ...
    PHP = ...
    PROTO = ...
    PYTHON = ...
    RST = ...
    RUBY = ...
    RUST = ...
    SCALA = ...
    SWIFT = ...
    MARKDOWN = ...
    LATEX = ...
    HTML = ...
    SOL = ...
    CSHARP = ...
    COBOL = ...
    C = ...
    LUA = ...
    PERL = ...
    HASKELL = ...
    ELIXIR = ...
    POWERSHELL = ...

@dataclass(frozen=True)
class Tokenizer:
    """Tokenizer data class."""

    chunk_overlap: int
    tokens_per_chunk: int
    decode: Callable[[list[int]], str]
    encode: Callable[[str], list[int]]
    ...

def split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> list[str]:
    """Split incoming text and return chunks using tokenizer."""
    ...
