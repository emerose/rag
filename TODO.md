    1. Refactor RAGConfig into an immutable @dataclass(frozen=True) and move all runtime-mutable flags (e.g., progress_callback, log_callback) into a separate, non-frozen “RuntimeOptions” object—this stops silent mutation bugs and makes the config hashable for caching.
    2.	Replace the synchronous OpenAIEmbeddings.embed_documents() calls inside ThreadPoolExecutor with the OpenAI Python client’s async interface (await client.embeddings.create) and an asyncio‐bounded semaphore to throttle requests—this removes thread-overhead, respects rate-limits, and lets you reuse the same event loop in the TUI.
    3.	Move all file-system work (hashing, MIME detection, PDF splitting, etc.) into an isolated ingest.py module and let rag_engine.py focus purely on vector-store + QA—clear separation of concerns will simplify unit-testing and future back-ends (e.g., Chroma, Qdrant).
    4.	Add an incremental-indexing guard: store each document’s SHA-256 + chunking params in a tiny SQLite table (index_meta.db); skip re-embedding if neither the file hash nor the chunking parameters changed.
    5.	Replace the ad-hoc lock files with filelock’s context-manager in a with block (with FileLock(path, timeout): …) so locks are always released, even on SIGINT.
    6.	Swap the hand-rolled log setup for structlog + RichHandler; emit JSON when --json is passed to the CLI so logs can feed directly into logging-agent pipelines.
    7.	In tui.py, run the long-running index/QA tasks in asyncio.create_task() and call await task inside on_idle(); remove the timer-based shutdown hack and instead call self.exit() from the task when it finishes—this fixes the “doesn’t quit after indexing” race.
    8.	Expose a --max-workers option on the CLI/TUI that defaults to min(32, os.cpu_count() + 4); propagate it to both the ThreadPool (if you keep it) and the async-semaphore, giving users control over throughput and API-cost.
    9.	Guard all OpenAI calls with exponential back-off (tenacity.retry) on RateLimitError and APIError; log retries at DEBUG and surfacing a single WARNING on final failure.
    10.	Write unit tests with pytest-asyncio, using respx or openai-mock to fake OpenAI responses, and pytest-tmpdir to exercise incremental indexing without touching the real cache.
    11.	Package the project with pyproject.toml + hatch; add an entry-point group ([project.scripts] rag = "rag.cli:app") so users get a single rag command instead of calling python cli.py.
    12.	Adopt ruff + mypy --strict in CI; fix the missing return annotations and the untyped Any parameters (progress_callback, log_callback, batch: list[Any], etc.) to catch bugs before runtime.
    13.	De-duplicate identical CSS strings in tui.py by extracting them to a styles.tcss file and loading via StaticCSS, shrinking the source and allowing theme overrides.
    14.	Give RAGEngine.answer() a pure-function signature (question: str, k: int = 4 -> str) and let the CLI/TUI handle prompt styling—this makes the engine reusable in other contexts (web API, batch mode).
    15.	Add a poetry export -f requirements.txt --without-hashes (or pip-tools) generated lockfile to pin versions of LangChain/LlamaIndex/OpenAI that frequently break compatibility.
    16.	Document the public API with Sphinx and auto-publish to GitHub Pages on main pushes; include a “How to plug in a different vector store” example to future-proof against FAISS limitations.
    17.	Provide a --dry-run flag that walks the docs and prints planned actions (chunks, bytes, price estimate) without hitting OpenAI—crucial for cost-conscious users.
    18.	Add a cleanup_orphaned_chunks() helper that deletes cached vector stores whose source files were removed, to keep .cache/ from growing unbounded.
    19.	Introduce semantic-aware chunking (e.g., RecursiveCharacterTextSplitter or MarkdownHeaderTextSplitter) instead of fixed token windows—this improves answer relevance and reduces token usage.
    20.	Write a short CONTRIBUTING.md explaining the coding standards, pre-commit hooks, and how to run the TUI locally; this lowers the barrier for outside contributions and speeds your own future onboarding.
